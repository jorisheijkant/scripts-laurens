{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama test\n",
    "\n",
    "Test your locally running version of Ollama with the script below. \n",
    "\n",
    "First, import the needed modules (in this case only `ollama`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up prompt\n",
    "\n",
    "Below, set up the system prompt, be as concise as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Je krijgt een tweet te zien, laat weten of er in deze tweet sprake is van haat of niet.\n",
    "De tweet is in het Nederlands. Je kijkt specifiek naar misoygne termen en homohaat. \n",
    "Geef een Boolean terug, dus False of True. Geef alleen de boolean terug.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up test data\n",
    "\n",
    "Use a piece of data to test your prompt and the connection to Ollama. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweet = \"Ik hoop dat die vadsige teringhond eens van tv verdwijnt.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Ollama module code\n",
    "\n",
    "The code below creates a message array. First there's the system message with the prompt, then a user message with the tweet to be labeled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": prompt\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": test_tweet\n",
    "    }\n",
    "]\n",
    "\n",
    "response = ollama.chat(model='llama3.2', messages=messages)\n",
    "\n",
    "try:\n",
    "    ollama_response = response['message']['content']\n",
    "    print(ollama_response)\n",
    "except Exception as e:\n",
    "    print(f\"Failing to parse Ollama answer: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
